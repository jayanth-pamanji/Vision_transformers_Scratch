{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "bqe5NqiTUl14"
      },
      "outputs": [],
      "source": [
        "#import libraries\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#variables\n",
        "batch_size = 64\n",
        "img_size = 28\n",
        "patch_size = 7\n",
        "num_channels = 1\n",
        "num_patches = (img_size // patch_size) ** 2\n",
        "num_heads = 1\n",
        "embed_dim = 16\n",
        "mlp_dim = 16\n",
        "transformer_units = 1"
      ],
      "metadata": {
        "id": "0pLVqs7CVobs"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Python image library (PIL) to tensor\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor()])"
      ],
      "metadata": {
        "id": "b4TOOw1jUtw5"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#load dataset\n",
        "trainset = torchvision.datasets.MNIST(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "valset = torchvision.datasets.MNIST(root='./data', train=False,\n",
        "                                        download=True, transform=transform)"
      ],
      "metadata": {
        "id": "yeAjyrD7Uwaw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00879d62-4c42-48a0-8057-067b213ace4d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 18.8MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 514kB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 3.95MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 4.60MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#create train and val batches\n",
        "train_data = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "                                          shuffle=True)\n",
        "val_data = torch.utils.data.DataLoader(valset, batch_size=batch_size,\n",
        "                                          shuffle=False)"
      ],
      "metadata": {
        "id": "PDI58WgqVP5v"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PatchEmbedding(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.patch_embed = nn.Conv2d(num_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.patch_embed(x)\n",
        "        x = x.flatten(2)\n",
        "        x = x.transpose(1,2)\n",
        "        return x"
      ],
      "metadata": {
        "id": "raej4gAFWG0O"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerArchitecture(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.layer_norm_1 = nn.LayerNorm(embed_dim)\n",
        "        self.self_attention = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n",
        "        self.layer_norm_2 = nn.LayerNorm(embed_dim)\n",
        "        self.multi_layer_perceptron = nn.Sequential(\n",
        "            nn.Linear(embed_dim, mlp_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(mlp_dim, embed_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual_1 = x\n",
        "        attention_output = self.self_attention(self.layer_norm_1(x),self.layer_norm_1(x),self.layer_norm_1(x))[0]\n",
        "        x = attention_output + residual_1\n",
        "        residual_2 = x\n",
        "        mlp_output = self.multi_layer_perceptron(self.layer_norm_2(x))\n",
        "        x = mlp_output + residual_2\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "Kbu5iXfycw4o"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VisionTransformer(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.patch_embedding = PatchEmbedding()\n",
        "        self.cls_token = nn.Parameter(torch.randn(1,1,embed_dim))\n",
        "        self.pos_embed = nn.Parameter(torch.randn(1, (img_size // patch_size) ** 2 + 1, embed_dim))\n",
        "        self.transformer_layers = nn.Sequential(*[TransformerArchitecture() for _ in range(transformer_units)])\n",
        "\n",
        "        self.mlp_head = nn.Sequential(\n",
        "            nn.LayerNorm(embed_dim),\n",
        "            nn.Linear(embed_dim, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self,x):\n",
        "        x = self.patch_embedding(x)\n",
        "        B = x.size(0)\n",
        "\n",
        "        cls_tokens = self.cls_token.expand(B , -1, -1)\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "        x = x + self.pos_embed\n",
        "        x = self.transformer_layers(x)\n",
        "        x = x[:,0]\n",
        "        x = self.mlp_head(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "SxNgMYCcfMwm"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = VisionTransformer().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "jf7edzGPhTI-"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "9WqaDY-dmn7v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(5):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct_epoch = 0\n",
        "    total_epoch = 0\n",
        "    print(f\"\\nEpoch {epoch+1}\")\n",
        "\n",
        "    for batch_idx, (images, labels) in enumerate(train_data):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss+=loss.item()\n",
        "        preds = outputs.argmax(dim=1)\n",
        "\n",
        "        correct = (preds == labels).sum().item()\n",
        "        accuracy = 100.0 * correct / labels.size(0)\n",
        "\n",
        "        correct_epoch += correct\n",
        "        total_epoch += labels.size(0)\n",
        "\n",
        "        if batch_idx % 100 == 0:\n",
        "            print(f\"  Batch {batch_idx+1:3d}: Loss = {loss.item():.4f}, Accuracy = {accuracy:.2f}%\")\n",
        "\n",
        "    epoch_acc = 100.0 * correct_epoch / total_epoch\n",
        "    print(f\"==> Epoch {epoch+1} Summary: Total Loss = {total_loss:.4f}, Accuracy = {epoch_acc:.2f}%\")"
      ],
      "metadata": {
        "id": "_5-pmMHqhaUW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "201d1677-0908-441f-b634-ec2118e0c6a6"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1\n",
            "  Batch   1: Loss = 2.7004, Accuracy = 7.81%\n",
            "  Batch 101: Loss = 1.3421, Accuracy = 64.06%\n",
            "  Batch 201: Loss = 0.8963, Accuracy = 67.19%\n",
            "  Batch 301: Loss = 0.9534, Accuracy = 71.88%\n",
            "  Batch 401: Loss = 0.5637, Accuracy = 81.25%\n",
            "  Batch 501: Loss = 0.7103, Accuracy = 73.44%\n",
            "  Batch 601: Loss = 0.6156, Accuracy = 79.69%\n",
            "  Batch 701: Loss = 0.7106, Accuracy = 79.69%\n",
            "  Batch 801: Loss = 0.5765, Accuracy = 82.81%\n",
            "  Batch 901: Loss = 0.6450, Accuracy = 78.12%\n",
            "==> Epoch 1 Summary: Total Loss = 792.6530, Accuracy = 71.67%\n",
            "\n",
            "Epoch 2\n",
            "  Batch   1: Loss = 0.9192, Accuracy = 68.75%\n",
            "  Batch 101: Loss = 0.5495, Accuracy = 81.25%\n",
            "  Batch 201: Loss = 0.5850, Accuracy = 81.25%\n",
            "  Batch 301: Loss = 0.2203, Accuracy = 92.19%\n",
            "  Batch 401: Loss = 0.5697, Accuracy = 82.81%\n",
            "  Batch 501: Loss = 0.3861, Accuracy = 84.38%\n",
            "  Batch 601: Loss = 0.5442, Accuracy = 82.81%\n",
            "  Batch 701: Loss = 0.6879, Accuracy = 81.25%\n",
            "  Batch 801: Loss = 0.6357, Accuracy = 81.25%\n",
            "  Batch 901: Loss = 0.4681, Accuracy = 82.81%\n",
            "==> Epoch 2 Summary: Total Loss = 467.0203, Accuracy = 84.14%\n",
            "\n",
            "Epoch 3\n",
            "  Batch   1: Loss = 0.5827, Accuracy = 75.00%\n",
            "  Batch 101: Loss = 0.5050, Accuracy = 82.81%\n",
            "  Batch 201: Loss = 0.4111, Accuracy = 82.81%\n",
            "  Batch 301: Loss = 0.4866, Accuracy = 84.38%\n",
            "  Batch 401: Loss = 0.4487, Accuracy = 87.50%\n",
            "  Batch 501: Loss = 0.8190, Accuracy = 73.44%\n",
            "  Batch 601: Loss = 0.7382, Accuracy = 79.69%\n",
            "  Batch 701: Loss = 0.4579, Accuracy = 85.94%\n",
            "  Batch 801: Loss = 0.3034, Accuracy = 92.19%\n",
            "  Batch 901: Loss = 0.1890, Accuracy = 96.88%\n",
            "==> Epoch 3 Summary: Total Loss = 392.2548, Accuracy = 86.69%\n",
            "\n",
            "Epoch 4\n",
            "  Batch   1: Loss = 0.3925, Accuracy = 87.50%\n",
            "  Batch 101: Loss = 0.4633, Accuracy = 85.94%\n",
            "  Batch 201: Loss = 0.5735, Accuracy = 81.25%\n",
            "  Batch 301: Loss = 0.2357, Accuracy = 90.62%\n",
            "  Batch 401: Loss = 0.4762, Accuracy = 87.50%\n",
            "  Batch 501: Loss = 0.3137, Accuracy = 87.50%\n",
            "  Batch 601: Loss = 0.2608, Accuracy = 93.75%\n",
            "  Batch 701: Loss = 0.5822, Accuracy = 85.94%\n",
            "  Batch 801: Loss = 0.3688, Accuracy = 89.06%\n",
            "  Batch 901: Loss = 0.4503, Accuracy = 82.81%\n",
            "==> Epoch 4 Summary: Total Loss = 357.5705, Accuracy = 87.92%\n",
            "\n",
            "Epoch 5\n",
            "  Batch   1: Loss = 0.4123, Accuracy = 90.62%\n",
            "  Batch 101: Loss = 0.2337, Accuracy = 95.31%\n",
            "  Batch 201: Loss = 0.2840, Accuracy = 89.06%\n",
            "  Batch 301: Loss = 0.3026, Accuracy = 92.19%\n",
            "  Batch 401: Loss = 0.2762, Accuracy = 87.50%\n",
            "  Batch 501: Loss = 0.4519, Accuracy = 82.81%\n",
            "  Batch 601: Loss = 0.2080, Accuracy = 93.75%\n",
            "  Batch 701: Loss = 0.4858, Accuracy = 85.94%\n",
            "  Batch 801: Loss = 0.3444, Accuracy = 90.62%\n",
            "  Batch 901: Loss = 0.2693, Accuracy = 89.06%\n",
            "==> Epoch 5 Summary: Total Loss = 338.3793, Accuracy = 88.46%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in val_data:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        preds = outputs.argmax(dim=1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "test_acc = 100.0 * correct / total\n",
        "print(f\"\\n==> Val Accuracy: {test_acc:.2f}%\")\n"
      ],
      "metadata": {
        "id": "ULQNNN-wsXXK",
        "outputId": "cc3dde4b-6e77-440c-ce9b-1e65e1001030",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==> Val Accuracy: 89.61%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7d3067a0",
        "outputId": "4c6e105c-21ef-4cda-a8d3-d124221d2b84",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Get one batch from the validation data\n",
        "images, labels = next(iter(val_data))\n",
        "\n",
        "# Print the shape of the images tensor\n",
        "print(\"Shape of images in a batch:\", images.shape)\n",
        "patch_embed = nn.Conv2d(num_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "embedded_data = patch_embed(images)\n",
        "embedded_data = embedded_data.flatten(2)\n",
        "embedded_data = embedded_data.transpose(1,2)\n",
        "print(\"Shape of embedded data:\", embedded_data.shape)\n",
        "print(torch.randn(1,1,embed_dim).shape)\n",
        "\n",
        "# You can also print the shape of the labels tensor if you'd like\n",
        "# print(\"Shape of labels in a batch:\", labels.shape)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of images in a batch: torch.Size([64, 1, 28, 28])\n",
            "Shape of embedded data: torch.Size([64, 16, 16])\n",
            "torch.Size([1, 1, 16])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TErvBBALdOcG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}